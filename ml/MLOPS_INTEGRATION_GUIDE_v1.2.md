# MLOps Integration Guide v1.2
## ğŸš€ Complete MLOps Pipeline for RagaSense

This guide covers the complete MLOps pipeline integration for the Optimized Working MVE v1.2 model, including experiment tracking, model management, and deployment.

## ğŸ“‹ Table of Contents

1. [MLOps Overview](#mlops-overview)
2. [MLflow Integration](#mlflow-integration)
3. [Experiment Tracking](#experiment-tracking)
4. [Model Management](#model-management)
5. [Deployment Pipeline](#deployment-pipeline)
6. [Monitoring & Observability](#monitoring--observability)
7. [Best Practices](#best-practices)

## ğŸ”§ MLOps Overview

### **What is MLOps?**
MLOps (Machine Learning Operations) is a set of practices that combines Machine Learning and DevOps to standardize and streamline the ML lifecycle. For RagaSense, this includes:

- **Experiment Tracking** - Track all training runs, hyperparameters, and metrics
- **Model Versioning** - Version control for ML models
- **Model Registry** - Centralized model storage and management
- **Model Deployment** - Automated model serving and deployment
- **Monitoring** - Track model performance in production
- **Reproducibility** - Ensure consistent results across environments

### **MLOps Tools Used**
- **MLflow** - Experiment tracking and model management
- **PyTorch** - Model framework
- **Docker** - Containerization (for deployment)
- **Git** - Version control for code
- **CI/CD** - Continuous integration/deployment

## ğŸ”¬ MLflow Integration

### **Setup MLflow**

```bash
# Install MLflow
pip install mlflow[extras]

# Start MLflow server
mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns
```

### **MLflow Components**

1. **Tracking Server** - Stores experiments, runs, and metadata
2. **Model Registry** - Manages model versions and stages
3. **Artifact Store** - Stores model files, plots, and other artifacts
4. **UI** - Web interface for visualization

### **Experiment Structure**

```
RagaSense_Optimized_Working_MVE_v1.2/
â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ run_001/  # Training run 1
â”‚   â”œâ”€â”€ run_002/  # Training run 2
â”‚   â””â”€â”€ run_003/  # Training run 3
â”œâ”€â”€ models/
â”‚   â””â”€â”€ optimized_working_mve_v1.2/
â””â”€â”€ artifacts/
    â”œâ”€â”€ plots/
    â”œâ”€â”€ reports/
    â””â”€â”€ checkpoints/
```

## ğŸ“Š Experiment Tracking

### **What Gets Tracked**

#### **Hyperparameters**
```python
hyperparams = {
    "model.d_model": 384,
    "model.n_heads": 6,
    "model.n_layers": 4,
    "training.batch_size": 4,
    "training.learning_rate": 2e-4,
    "training.weight_decay": 1e-4,
    "training.epochs": 50,
    "training.label_smoothing": 0.1,
    "audio.sr": 22050,
    "audio.n_mels": 128,
    "audio.n_fft": 2048,
    "audio.hop_length": 512
}
```

#### **Metrics**
- **Training Metrics**: Loss, Accuracy, F1 Score
- **Validation Metrics**: Validation Loss, Validation Accuracy
- **System Metrics**: GPU Usage, Memory Usage, Training Time
- **Model Metrics**: Model Size, Inference Time

#### **Artifacts**
- **Model Files**: PyTorch model checkpoints
- **Plots**: Training curves, confusion matrices, performance plots
- **Reports**: Training reports, evaluation results
- **Logs**: Training logs, error logs

### **Viewing Experiments**

```bash
# Start MLflow UI
mlflow ui --backend-store-uri ./mlruns

# View in browser
open http://localhost:5000
```

## ğŸ—ï¸ Model Management

### **Model Registry**

```python
# Register model
mlflow.pytorch.log_model(
    pytorch_model=model,
    artifact_path="optimized_working_mve_v1.2",
    registered_model_name="optimized_working_mve_v1.2"
)
```

### **Model Stages**

- **None** - Model is registered but not staged
- **Staging** - Model is ready for testing
- **Production** - Model is deployed to production
- **Archived** - Model is no longer used

### **Model Versioning**

```bash
# List models
mlflow models list

# Get model details
mlflow models describe -m optimized_working_mve_v1.2

# List model versions
mlflow models versions list -m optimized_working_mve_v1.2
```

## ğŸš€ Deployment Pipeline

### **Model Serving**

```bash
# Serve model locally
mlflow models serve -m ./mlruns/0/[run_id]/artifacts/optimized_working_mve_v1.2

# Serve with specific port
mlflow models serve -m ./mlruns/0/[run_id]/artifacts/optimized_working_mve_v1.2 --port 5001
```

### **Docker Deployment**

```dockerfile
# Dockerfile for model serving
FROM python:3.9-slim

WORKDIR /app

# Install dependencies
COPY requirements_v1.2.txt .
RUN pip install -r requirements_v1.2.txt

# Copy model
COPY mlruns/ ./mlruns/

# Expose port
EXPOSE 5000

# Start MLflow server
CMD ["mlflow", "models", "serve", "-m", "./mlruns/0/[run_id]/artifacts/optimized_working_mve_v1.2", "--host", "0.0.0.0"]
```

### **API Endpoints**

```python
# Example API usage
import requests

# Predict endpoint
response = requests.post(
    "http://localhost:5000/invocations",
    json={
        "inputs": {
            "mel_spectrogram": mel_spec_data,
            "f0_features": f0_data,
            "tradition_id": tradition_id
        }
    }
)

result = response.json()
```

## ğŸ“ˆ Monitoring & Observability

### **Model Performance Monitoring**

```python
# Log model performance
mlflow.log_metrics({
    "inference_time": inference_time,
    "memory_usage": memory_usage,
    "prediction_confidence": confidence,
    "error_rate": error_rate
})
```

### **Data Drift Detection**

```python
# Monitor input data distribution
mlflow.log_metrics({
    "input_mean": input_mean,
    "input_std": input_std,
    "input_distribution_shift": drift_score
})
```

### **Alerting**

```python
# Set up alerts for model performance
if accuracy < threshold:
    mlflow.log_metric("alert_triggered", 1)
    # Send alert notification
```

## ğŸ¯ Best Practices

### **Experiment Organization**

1. **Use Descriptive Names**: `optimized_working_mve_v1.2_20241201_143022`
2. **Tag Runs**: Add tags for easy filtering
3. **Document Changes**: Log what changed between runs
4. **Compare Runs**: Use MLflow UI to compare different experiments

### **Model Management**

1. **Version Everything**: Models, data, code, and environment
2. **Test Before Production**: Use staging environment
3. **Monitor Performance**: Track metrics in production
4. **Rollback Capability**: Keep previous model versions

### **Reproducibility**

1. **Pin Dependencies**: Use exact version numbers
2. **Environment Files**: Include conda/pip environment files
3. **Random Seeds**: Set random seeds for reproducibility
4. **Documentation**: Document all assumptions and decisions

### **Security**

1. **Access Control**: Limit access to model registry
2. **Data Privacy**: Ensure sensitive data is not logged
3. **Model Security**: Validate model inputs and outputs
4. **Audit Trail**: Log all model access and changes

## ğŸ”§ Troubleshooting

### **Common Issues**

#### **MLflow Server Not Starting**
```bash
# Check if port is in use
lsof -i :5000

# Use different port
mlflow ui --backend-store-uri ./mlruns --port 5001
```

#### **Model Loading Issues**
```bash
# Check model artifacts
ls -la ./mlruns/0/[run_id]/artifacts/optimized_working_mve_v1.2/

# Verify model format
python -c "import torch; torch.load('./mlruns/0/[run_id]/artifacts/optimized_working_mve_v1.2/model.pkl')"
```

#### **Permission Issues**
```bash
# Fix permissions
chmod -R 755 ./mlruns
chown -R $USER:$USER ./mlruns
```

### **Performance Optimization**

1. **Use GPU**: Enable MPS/CUDA for faster training
2. **Batch Processing**: Process multiple samples together
3. **Model Quantization**: Reduce model size for deployment
4. **Caching**: Cache frequently used data

## ğŸ“š Additional Resources

### **MLflow Documentation**
- [MLflow Official Docs](https://mlflow.org/docs/latest/index.html)
- [MLflow PyTorch Integration](https://mlflow.org/docs/latest/models.html#pytorch-pytorch-model)
- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)

### **MLOps Best Practices**
- [MLOps Principles](https://ml-ops.org/)
- [Model Versioning](https://dvc.org/doc/use-cases/versioning-data-and-model-files)
- [Model Deployment](https://www.kubeflow.org/docs/components/serving/)

## ğŸ‰ Success Metrics

### **MLOps KPIs**

- âœ… **Experiment Tracking**: 100% of runs tracked
- âœ… **Model Versioning**: All models versioned
- âœ… **Reproducibility**: Consistent results across runs
- âœ… **Deployment Time**: < 5 minutes from training to serving
- âœ… **Model Performance**: > 87% accuracy maintained
- âœ… **Monitoring**: Real-time performance tracking

### **Business Impact**

- **Faster Iteration**: Reduced time from experiment to production
- **Better Quality**: Consistent model performance
- **Reduced Risk**: Easy rollback and monitoring
- **Team Collaboration**: Shared experiments and models
- **Compliance**: Audit trail and version control

---

**Ready to implement MLOps for RagaSense!** ğŸš€

This guide provides everything needed to set up a complete MLOps pipeline for the Optimized Working MVE v1.2 model, ensuring professional-grade machine learning operations.