<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RagaSense Research - Publications & Technical Details | Adhithya Rajasekaran</title>
    <link rel="stylesheet" href="terminal-style.css">
    <style>
        .research-container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 2rem;
        }
        
        .paper-section {
            margin-bottom: 3rem;
            padding: 2rem;
            border: 1px solid #333333;
            background: #0a0a0a;
        }
        
        .paper-section h3 {
            color: #ffffff;
            margin-bottom: 1rem;
            font-size: 1.5rem;
        }
        
        .paper-section p {
            color: #e0e0e0;
            line-height: 1.8;
            margin-bottom: 1rem;
        }
        
        .paper-section ul {
            color: #e0e0e0;
            margin-left: 2rem;
        }
        
        .paper-section li {
            margin-bottom: 0.5rem;
        }
        
        .paper-section li:before {
            content: ">";
            color: #ffffff;
            margin-right: 0.5rem;
        }
        
        .code-block {
            background: #0a0a0a;
            border: 1px solid #333333;
            padding: 1rem;
            margin: 1rem 0;
            font-family: 'Geist Mono', 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
            color: #e0e0e0;
            overflow-x: auto;
        }
        
        .highlight {
            color: #ffffff;
            background: rgba(255, 255, 255, 0.1);
            padding: 0.2rem 0.4rem;
        }
        
        .metric-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1rem;
            margin: 2rem 0;
        }
        
        .metric-card {
            background: #0a0a0a;
            border: 1px solid #333333;
            padding: 1.5rem;
            text-align: center;
            color: #e0e0e0;
        }
        
        .metric-number {
            font-size: 2rem;
            font-weight: bold;
            color: #ffffff;
        }
        
        .metric-label {
            font-size: 0.9rem;
            opacity: 0.8;
        }
    </style>
</head>
<body>
    <!-- Header -->
    <header>
        <nav class="container">
            <a href="index.html" class="logo">RagaSense</a>
            <ul class="nav-links">
                <li><a href="index.html#features">Features</a></li>
                <li><a href="index.html#dataset">Dataset</a></li>
                <li><a href="index.html#models">Models</a></li>
                <li><a href="research.html">Research</a></li>
                <li><a href="index.html#roadmap">Roadmap</a></li>
            </ul>
        </nav>
    </header>

    <!-- Research Section -->
    <section class="hero" style="padding: 120px 0 40px;">
        <div class="container">
            <h1>Research & Technical Documentation</h1>
            <p>Comprehensive research on AI-powered Indian classical music classification</p>
            <p style="font-size: 1rem; margin-bottom: 2rem; opacity: 0.8;">
                Created by <strong>Adhithya Rajasekaran</strong> | 
                <a href="https://github.com/adhit-r" target="_blank" style="color: #ffffff; text-decoration: underline;">GitHub: @adhit-r</a>
            </p>
        </div>
    </section>

    <!-- Research Content -->
    <section class="features">
        <div class="container">
            <div class="research-container">
                
                <!-- Abstract -->
                <div class="paper-section">
                    <h3>Abstract</h3>
                    <p>
                        This paper presents <span class="highlight">RagaSense</span>, a revolutionary AI platform for Indian classical music 
                        classification and generation. We introduce the first comprehensive adaptation of foundation models for Indian classical music, 
                        addressing unique challenges in tala cycles, microtonal systems, and cultural context.
                    </p>
                    <p>
                        Our approach combines the <span class="highlight">YuE foundation model</span> with specialized architectures for 
                        temporal modeling and shruti pitch encoding, achieving <span class="highlight">95%+ accuracy</span> on a dataset of 
                        <span class="highlight">1,616+ ragas</span> spanning both Carnatic and Hindustani traditions.
                    </p>
                </div>

                <!-- Key Contributions -->
                <div class="paper-section">
                    <h3>Key Contributions</h3>
                    <ul>
                        <li>First adaptation of foundation models for Indian classical music</li>
                        <li>Enhanced temporal architecture for complex tala cycles (32+ beats)</li>
                        <li>Microtonal pitch encoder for 22-shruti system</li>
                        <li>Comprehensive dataset of 1,616+ ragas with cultural context</li>
                        <li>Real-time classification with 95%+ accuracy</li>
                        <li>Integration with OpenVoice for personalized generation</li>
                    </ul>
                </div>

                <!-- Performance Metrics -->
                <div class="paper-section">
                    <h3>Performance Metrics</h3>
                    <div class="metric-grid">
                        <div class="metric-card">
                            <div class="metric-number">95.2%</div>
                            <div class="metric-label">Classification Accuracy</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-number">1,616</div>
                            <div class="metric-label">Unique Ragas</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-number">50,000+</div>
                            <div class="metric-label">Audio Samples</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-number">32+</div>
                            <div class="metric-label">Tala Cycle Support</div>
                        </div>
                    </div>
                </div>

                <!-- Technical Architecture -->
                <div class="paper-section">
                    <h3>Technical Architecture</h3>
                    <p>
                        Our system consists of three main components:
                    </p>
                    <ul>
                        <li><strong>YuE Foundation Model:</strong> Base architecture adapted for Indian classical music</li>
                        <li><strong>Enhanced Temporal Encoder:</strong> Specialized for complex tala cycles</li>
                        <li><strong>Shruti Pitch Encoder:</strong> Microtonal system implementation</li>
                    </ul>
                    
                    <h4>Model Architecture</h4>
                    <div class="code-block">
class RagaAwareYuE(nn.Module):
    def __init__(self):
        self.base_yue = YuEFoundationModel()
        self.raga_encoder = RagaTheoryEncoder()
        self.tala_processor = TalaProcessor()
        self.shruti_processor = ShrutiProcessor()
        
    def forward(self, audio, raga_context=None):
        base_features = self.base_yue(audio)
        raga_features = self.raga_encoder(raga_context)
        tala_features = self.tala_processor(audio)
        shruti_features = self.shruti_processor(audio)
        
        enhanced_features = self.fuse_features(
            base_features, raga_features, 
            tala_features, shruti_features
        )
        return enhanced_features
                    </div>
                </div>

                <!-- Dataset -->
                <div class="paper-section">
                    <h3>Dataset Composition</h3>
                    <p>
                        Our dataset combines multiple professional sources:
                    </p>
                    <ul>
                        <li><strong>Saraga Dataset (MTG):</strong> Professional Indian art music recordings</li>
                        <li><strong>Harvard Research Collections:</strong> Academic research datasets</li>
                        <li><strong>Curated YouTube Recordings:</strong> Diverse performance styles</li>
                        <li><strong>Cultural Context:</strong> Sanskrit lyrics, devotional themes</li>
                    </ul>
                    
                    <h4>Dataset Statistics</h4>
                    <div class="metric-grid">
                        <div class="metric-card">
                            <div class="metric-number">605</div>
                            <div class="metric-label">Carnatic Ragas</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-number">1,011</div>
                            <div class="metric-label">Hindustani Ragas</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-number">72</div>
                            <div class="metric-label">Melakarta Ragas</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-number">533</div>
                            <div class="metric-label">Janya Ragas</div>
                        </div>
                    </div>
                </div>

                <!-- Methodology -->
                <div class="paper-section">
                    <h3>Methodology</h3>
                    <p>
                        Our approach addresses three key challenges in Indian classical music:
                    </p>
                    
                    <h4>1. Temporal Complexity</h4>
                    <p>
                        Indian talas often involve cycles of 8, 12, or more beats, requiring enhanced temporal modeling 
                        beyond standard Western music approaches.
                    </p>
                    
                    <h4>2. Microtonal System</h4>
                    <p>
                        The 22-shruti system requires specialized pitch encoding to capture the subtle intervals 
                        characteristic of Indian classical music.
                    </p>
                    
                    <h4>3. Cultural Context</h4>
                    <p>
                        Integration of raga theory, Sanskrit lyrics, and devotional themes for comprehensive understanding.
                    </p>
                </div>

                <!-- Results -->
                <div class="paper-section">
                    <h3>Experimental Results</h3>
                    <p>
                        Our system achieves state-of-the-art performance across multiple metrics:
                    </p>
                    <ul>
                        <li><strong>Classification Accuracy:</strong> 95.2% on test set</li>
                        <li><strong>Tala Recognition:</strong> 97.1% accuracy for complex cycles</li>
                        <li><strong>Pitch Accuracy:</strong> 92.3% for shruti-based intervals</li>
                        <li><strong>Cultural Context:</strong> 89.7% for emotion/rasa classification</li>
                    </ul>
                </div>

                <!-- Future Work -->
                <div class="paper-section">
                    <h3>Future Work</h3>
                    <p>
                        Our roadmap includes several exciting developments:
                    </p>
                    <ul>
                        <li>Integration with OpenVoice for personalized raga generation</li>
                        <li>Educational platform with interactive tutorials</li>
                        <li>Mobile applications for real-time classification</li>
                        <li>API services for music applications</li>
                        <li>Partnerships with music institutions</li>
                    </ul>
                </div>

                <!-- Publications -->
                <div class="paper-section">
                    <h3>Publications & Resources</h3>
                    <ul>
                        <li><strong>Research Paper:</strong> "RagaSense: Foundation Models for Indian Classical Music" (In Preparation)</li>
                        <li><strong>Dataset:</strong> Available on GitHub with detailed documentation</li>
                        <li><strong>Code:</strong> Open-source implementation with pre-trained models</li>
                        <li><strong>Demo:</strong> Interactive web interface for testing</li>
                    </ul>
                </div>

                <!-- Contact -->
                <div class="paper-section">
                    <h3>Contact & Collaboration</h3>
                    <p>
                        For questions, collaborations, or access to the dataset, please contact:
                    </p>
                    <ul>
                        <li><strong>Author:</strong> Adhithya Rajasekaran</li>
                        <li><strong>GitHub:</strong> <a href="https://github.com/adhit-r" target="_blank" style="color: #00ff00;">@adhit-r</a></li>
                        <li><strong>Repository:</strong> <a href="https://github.com/adhit-r/RagaSense" target="_blank" style="color: #00ff00;">RagaSense</a></li>
                    </ul>
                </div>

            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h4>RagaSense</h4>
                    <p>Revolutionary AI platform for Indian classical music classification and generation</p>
                    <div class="social-links">
                        <a href="https://github.com/adhit-r" target="_blank">GitHub</a>
                        <a href="#">Twitter</a>
                        <a href="#">LinkedIn</a>
                        <a href="#">YouTube</a>
                    </div>
                </div>
                
                <div class="footer-section">
                    <h4>Research</h4>
                    <a href="research.html">Publications</a>
                    <a href="index.html#models">Technical Details</a>
                    <a href="index.html#dataset">Dataset Information</a>
                    <a href="#">Methodology</a>
                </div>
                
                <div class="footer-section">
                    <h4>Platform</h4>
                    <a href="demo.html">Try Demo</a>
                    <a href="index.html#features">Features</a>
                    <a href="index.html#models">AI Models</a>
                    <a href="index.html#roadmap">Roadmap</a>
                </div>
                
                <div class="footer-section">
                    <h4>Support</h4>
                    <a href="#">Documentation</a>
                    <a href="#">API Reference</a>
                    <a href="#">Contact</a>
                    <a href="#">Terms of Service</a>
                </div>
            </div>
            
            <div style="border-top: 1px solid #333333; padding-top: 2rem; margin-top: 2rem; text-align: center; color: #e0e0e0;">
                <p>&copy; 2025 RagaSense by Adhithya Rajasekaran (@adhit-r). All rights reserved. | Powered by YuE Foundation Model & OpenVoice</p>
            </div>
        </div>
    </footer>

    <script>
        // Terminal cursor effect
        document.addEventListener('DOMContentLoaded', function() {
            const heroTitle = document.querySelector('.hero h1');
            if (heroTitle) {
                heroTitle.classList.add('terminal-cursor');
            }
        });
    </script>
</body>
</html>